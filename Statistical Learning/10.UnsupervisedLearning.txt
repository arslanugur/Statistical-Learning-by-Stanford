10.1 Principal Components

You are analyzing a dataset where each observation is an age, height, length, and width of a particular turtle. 
You want to know if the data can be well described by fewer than four dimensions (maybe for plotting), 
so you decide to do Principal Component Analysis. Which of the following is most likely to be the loadings of the first Principal Component?
---> (.5, .5, .5, .5)
We know that options 1 and 4 cannot be right because the sum of the squared loadings exceeds 1. 
The second option is most likely correct because we expect all four variables to be positively correlated with each-other.
Note that it is fairly common for the loadings of the first principal component to all have the same sign.
In such a case, the principal component is often referred to as a size component.

##########################################################################################

10.2 Higher Order Principal Components

Suppose we a data set where each data point represents...
a single student's scores on a math test, a physics test, a reading comprehension test, and a vocabulary test.
We find the first two principal components, which capture 90% of the variability in the data, and interpret their loadings. 
We conclude that the first principal component represents overall academic ability, 
and the second represents a contrast between quantitative ability and verbal ability.
What loadings would be consistent with that interpretation? Choose all that apply.
---  (0.5, 0.5, 0.5, 0.5) and (0.71, 0.71, 0, 0)
---  (0.5, 0.5, 0.5, 0.5) and (0, 0, -0.71, -0.71)
---> (0.5, 0.5, 0.5, 0.5) and (0.5, 0.5, -0.5, -0.5)
---> (0.5, 0.5, 0.5, 0.5) and (-0.5, -0.5, 0.5, 0.5)
---  (0.71, 0.71, 0, 0) and (0, 0, 0.71, 0.71)
---  (0.71, 0, -0.71, 0) and (0, 0.71, 0, -0.71)
For the first two choices, the two loading vectors are not orthogonal.
For the fifth and sixth choices, the first set of loadings only has to do with two specific tests.
For the third and fourth pairs of loadings, the first component is proportional to average score,
and the second component measures the difference between the first pair of scores and the second pair of scores.

##########################################################################################

10.3 k-means Clustering

If we use k-means clustering, will we get the same cluster assignments for each point, whether or not we standardize the variables.
---> False
The points are assigned to centroids using Euclidean distance.
If we change the scaling of one variable, e.g. by dividing it by 10, then that variable will matter less in determining Euclidean distance.

##########################################################################################

10.4 Hierarchical Clustering

If we cut the dendrogram at a lower point, we will tend to get more clusters (and cannot get fewer clusters).
---> True
After cutting the dendrogram at threshold t, 
we keep all the joins with linkage distance less than t and discard the joins with larger linkage distance. 
Thus, decreasing the threshold gives us fewer joins, and thus more clusters. 
If, in decreasing the threshold, we don't cross a junction of the dendrogram, the number of clusters will remain the same.

##########################################################################################

10.5 Breast Cancer Example

In the heat map for breast cancer data, which of the following depended on the output of hierarchical clustering?
---> The ordering of the rows
---> The ordering of the columns
The dendrograms obtained from hierarchical clustering were used to order the rows and columns.
The coloring of the cells was based on gene expression, 
but without the hierarchical clustering step, the heat map would not have looked like anything meaningful.

##########################################################################################

10.R Unsupervised in R

Suppose we want to fit a linear regression, but the number of variables is much larger than the number of observations.
In some cases, we may improve the fit by reducing the dimension of the features before.
In this problem, we use a data set with n = 300 and p = 200, so we have more observations than variables, but not by much.
Load the data x, y, x.test, and y.test from 10.R.RData.
First, concatenate x and x.test using the rbind functions 
and perform a principal components analysis on the concatenated data frame (use the "scale=TRUE" option). 
To within 10% relative error, what proportion of the variance is explained by the first five principal components?
---> 0.3498565
use "vars = prcomp(rbind(x,x.test),scale=TRUE)$sdev^2"

The previous answer suggests that a relatively small number of "latent variables" account for a substantial fraction of the features' variability.
We might believe that these latent variables are more important than linear combinations of the features that have low variance.
We can try forgetting about the raw features and using the first five principal components (computed on rbind(x,x.test)) instead as low-dimensional derived features.
What is the mean-squared test error if we regress y on the first five principal components, and use the resulting model to predict y.test?
---> 0.9923
In the actual data generating model for this example, 
the features may be noisy proxies for a few latent variables that actually drive the response. 
This is not an uncommon situation when we have high-dimensional data.

Now, try an OLS linear regression of y on the matrix x. 
What is the mean squared predition error if we use the fitted model to predict y.test from x.test?
---> 3.90714
The mean squared error is worse because of the variance involved in fitting a very high dimensional model. 
As it turned out here, the large-variance directions of x turned out to be the important ones for predicting y. 
Note that this need not always be the case, but it often is.

##########################################################################################

Chapter 10 Quiz

K-Means is a seemingly complicated clustering algorithms. Here is a simpler one:
Given k, the number of clusters, and n, the number of observations, try all possible assignments of the n observations into k clusters. 
Then, select one of the assignments that minimizes Within-Cluster Variation as defined on page 30.
Assume that you implemented the most naive version of the above algorithm. 
Here, by naive we mean that you try all possible assignments even though some of them might be redundant 
(for example, the algorithm tries assigning all of the observations to cluster 1 
and it also tries to assign them all to cluster 2 even though those are effectively the same solution).
In terms of n and k, how many potential solutions will your algorithm try?
---> k^n
For each of the n observations we have k options for assignment. Each of the assignments is done independently, so k^n.
Note, the exponential explosion in the number of potential solutions is the reason we need to use greedy algorithms like K-Means in order to perform clustering.






