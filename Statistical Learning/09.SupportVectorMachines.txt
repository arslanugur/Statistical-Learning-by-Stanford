9.1 Optimal Separating Hyperplanes

If β is not a unit vector but instead has length 2, then ∑^p_j = 1β_jX_j is
---> twice the signed Euclidean distance from the separating hyperplane ∑^p_j = 1β_jX_j = 0
---  half the signed Euclidean distance from  X  to the separating hyperplane
---  exactly the signed Euclidean distance from the separating hyperplane
We know β^1 = 1/2β has length 1, so it is a unit vector in the same direction as β. 
Therefore, ∑^p_j = 1β_jX_j= 2∑^p_j = 1β^1_jX_j, where ∑^p_j=1β^1_jX_j is the Euclidean distance.

##########################################################################################

9.2 Support Vector Classifier

If we increase C (the error budget) in an SVM, do you expect the standard error of β to increase or decrease?
---> Decrease
Increasing C makes the margin "softer," so that the orientation of the separating hyperplane is influenced by more points.

##########################################################################################

9.3 Feature Expansion and the SVM

If no linear boundary can perfectly classify all the training data, this means we need to use a feature expansion. ---> False
As in any statistical problem, we will always do better on the training data if we use a feature expansion, 
but that doesn't mean we will improve the test error. 
Not all regression lines should perfectly interpolate all the training points, 
and not all classifiers should perfectly classify all the training data.

The computational effort required to solve a kernel support vector machine becomes greater and greater as the dimension of the basis increases.
---> False
The beauty of the "kernel trick" is that, even if there is an infinite-dimensional basis, 
we need only look at the n^2 inner products between training data points.

##########################################################################################

9.4 Example and Comparison with Logistic Regression

Recall that we obtain the ROC curve by classifying test points based on whether hatf(x) > t, and varying t.
How large is the AUC (area under the ROC curve) for a classifier based on a completely random function hatf(x) 
(that is, one for which the orderings of the hatf(x_i) are completely random)?
---> 0.5
If hatf(x) is completely random, then hatf(x_i) 
(and therefore the prediction for y_i) has nothing to do with y_i.
Thus, the true positive rate and the false positive rate are both equal to the overall positive rate, 
and the ROC curve hugs the 45-degree line

##########################################################################################

9.R SVMs in R

In this problem,
you will use simulation to evaluate (by Monte Carlo) the expected misclassification error rate given a particular generating model.
Let y_i be equally divided between classes 0 and 1, and let x_i ∈ R^10 be normally distributed.
Given y_i = 0, x_i ∼ N_10(0,I_10)
Given y_i = 1, x_i ∼ N_10(μ,I_10) with μ = (1,1,1,1,1,0,0,0,0,0)
The N_10 notation just means its a ten-dimensional Gaussian distribution; 
you can use the mvrnorm function in the MASS package to help generate the data. 
Now, we wanna know the expected test error rate if we fit an SVM to a sample of 50 random training points from class 1 and 50 more from class 0.  We can calculate this to high precision by 1) generating a random training sample to train on, 2) evaluating the number of mistakes we make on a large test set, and then 3) repeating (1-2) many times and averaging the error rate for each trial. 
Aside: in real life don't know the generating distribution, so we have to use resampling methods instead of the procedure described above.
For all of the following, please enter your error rate as a number between zero and 1 (e.g., 0.21 instead of 21 if the error rate is 21%)
---
Use svm in the e1071 package with the default settings (the default kernel is a radial kernel). What is the expected test error rate of this method (to within 10%)? ---> 0.16350
---
Now fit an svm with a linear kernel (kernel = "linear"). What is the expected test error rate to within 10%? ---> 0.15791
An svm with a linear kernel does a little better here, since the best decision boundary is truly linear.
---
What is the expected test error for logistic regression? (to within 10%)
(Don't worry if you get errors saying the logistic regression did not converge.)
---> 0.15750
Logistic regression is similar to SVM with a linear kernel.

##########################################################################################

Chapter 9 Quiz

Suppose that after our computer works for an hour to fit an SVM on a large data set, 
we notice that x_4, the feature vector for the fourth example, was recorded incorrectly 
(say, one of the decimal points is obviously in the wrong place).
However, your co-worker notices that the pair (x_4, y_4) did not turn out to be a support point in the original fit. 
He says there is no need to re-fit the SVM on the corrected data set, 
because changing the value of a non-support point can't possibly change the fit.
Is your co-worker correct? ---> No
When we change x_4, the fourth example might become a support point; if so, the fit may change. 
However, we could check whether x_4, y_4 is still not a support point even after correcting the value.
If so, then we really don't need to re-fit the model.

